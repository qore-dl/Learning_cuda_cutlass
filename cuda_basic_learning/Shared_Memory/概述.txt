宏观：元信息、训练/推理、占用资源的量（集群中跑了哪些模型，规模是什么，资源的比例）
中观：占用了哪些资源 （SM 利用率、利用率、关键OP）：OP 序列、kernel 序列预测
微观：主要性能瓶颈：op比例/kenrel launch 密度

第5章 共享内存与常量内存

本章内容：
1. 了解数据在共享内存中是如何被安排的
2. 掌握从二维共享内存到线性全局内存的索引转换
3. 解决不同访问模式中存储体的冲突
4. 在共享内存中缓存数据以减少对全局内存的访问
5. 使用共享内存避免非合并全局内存的访问
6. 理解常量缓存和只读缓存之间的差异
7. 使用线程束洗牌指令编程

在前面的章节中，已经介绍了几种全局内存的访问模式。通过安排全局内存访问模式（它们是合并的），
我们学会了如何实现良好的性能并且避免了浪费transaction。非对齐的内存访问是没有问题的，
因为现代的 GPU 硬件都有 L1 Cache，但在跨全局内存的非合并内存访问，仍然会导致带宽利用率不会达到最佳标准。
根据算法性质和相应的访问模式，非合并访问可能是无法避免的。然而，在许多情况下，使用共享内存来提高全局内存合并访问是有可能的。
共享内存是许多高性能计算应用程序的关键驱动力。
在本章中，将学习如何使用共享内存进行编程、数据在共享内存中如何被存储、数据元素是怎样使用不同的访问模式被映射到内存存储体中的。
还将掌握使用共享内存提高 kernel 函数性能的方法。

5.1 CUDA 共享内存概述
CUDA 中有两种类型的内存：
1. 板载内存
2. 片上内存

全局内存是较大的板载内存，具有相对较高的延迟。共享内存是较小的片上内存，
具有相对较低的延迟，并且共享内存可以提供比全局内存高得多的带宽。可以把它当做一个可编程管理的缓存。
共享内存通常的用途有：
1. 块内线程通信的通道
2. 用于全局内存数据的可编程管理的缓存
3. 高速暂存存储器，用于转换数据以优化全局内存访问模式
在本章中，将通过两个例子学习共享内存编程：归约 kernel 函数、矩阵转置kernel函数。

5.1.1 共享内存
共享内存（shared memory，SMEM）是 GPU 的一个关键部件。物理上，每个 SM 都有一个小的低延迟内存池。
这个内存池被当前正在该 SM 上执行的线程块中的所有线程所共享。
共享内存使同一个线程块中的线程能够互相协作，便于重用片上数据，并可以打打降低kernel函数所需的全局内存带宽。
由于共享内存中的内容是由应用程序显式管理的，所以它通常被描述为可编程管理的缓存。

------------------------------------             -------------------------------------
|SM0                                |            |SM1                                |
| |-------------------------------| |            | |-------------------------------| |
| |       寄存器                   | |            | |       寄存器                   | |
| |--------------------------------| |           | |--------------------------------| |
|  ^       ^          ^        ^     |           |  ^       ^          ^        ^     |
|  |       |          |        |     |           |  |       |          |        |     |
|  v       v          v        v     |           |  v       v          v        v     |
| |-----| |--------| |-----|  |----| |           | |-----| |--------| |-----|  |----| |
| |SMEM | |L1 Cache| |只读  |  |常量| |           | |SMEM | |L1 Cache| |只读  |  |常量| |
| ------- ---------- -------   ----- |           | ------- ---------- -------   ----- |
|-------------------------------------|          |-------------------------------------|
                    ^                                                  ^
                    |                                                  |
                    v                                                  v
|-----------------------------------------------------------------------------------------|
|二级缓存(L2 Cache)                                                                        |
|-----------------------------------------------------------------------------------------|                   
                    ^                                                  ^
                    |                                                  |
                    v                                                  v
|-----------------------------------------------------------------------------------------|
|DRAM                                                                                     |
|-----------------------------------------------------------------------------------------|

Fermi 和 Kepler GPU 具有相似的内存层次结构，不同的是 Kepler 包括一个额外的编译器导向缓存，它用于只读数据。
如上图所示，全局内存的所有load 和 store 请求都要经过 L2 Cache。这是 SM 单元之间数据统一的基本点。
注意，相较于二级缓存和全局内存，共享内存和一级缓存在物理上更接近 SM 。因此，共享内存相较于全局内存而言，延迟要低 20~30倍，而带宽高其大约 10 倍。

当每个block 开始执行时，会分配给它一定数量的共享内存。这个共享内存的地址空间被线程块中所有的线程共享。
它的内容和创建时所在的线程块具有相同的生命周期。每个wrap 发出共享内存访问请求。
在理想的情况下，每个被wrap共享内存访问的请求在一个transaction中完成。
最坏的情况下，每个共享内存访问的请求在32个不同的事务中顺序执行。
如果多个线程访问共享内存中的同一个字，一个线程读取该字后，通过多播把它发送给其他线程。
在以下几节中将介绍避免多transaction共享内存请求的更多细节。

共享内存被 SM 中的所有常驻 block 划分，因此，共享内存是限制设备并行性的关键资源。
一个 kernel 函数使用的共享内存越多，处于并发活跃状态的线程块就越少。

可编程管理的缓存
在 C 语言中，循环转换是一种常用的缓存优化方法。通过重新安排迭代顺序，循环转换可以在循环遍历的过程中提高缓存局部性。
在算法层面上，在考虑缓存大小的同时，需要手动调整循环，以实现更好的空间局部性。
缓存对于程序而言是透明的，编译器可以处理所有的数据移动，我们不能控制缓存的释放。

共享内存是一个可编程管理的缓存。当数据移动到共享内存中以及数据被释放时，我们对它有充分的控制权。
由于在 CUDA 中允许手动管理 shared memory，所以通过在数据布局上提供更多的细粒度控制和改善片上数据的移动，
使得对应用程序代码进行优化变得更简单了。

5.1.2 共享内存分配
有多种方法可以用来分配或声明由应用程序请求所决定的共享内存变量。可以静态或动态地分配共享内存变量。
在 CUDA 的源代码文件中，共享内存可以在一个本地的 CUDA kernel 函数或是一个全局的 CUDA kernel 函数中被声明。
CUDA 支持一维、二维和三维共享内存数组的声明。

共享内存变量可以用下列修饰符进行声明：
__shared__

下面的代码段静态声明了一个共享内存中的二维浮点数组。
如果在kernel函数中进行声明，那么这边变量的作用域就局限在该kernel中。如果在file的任何kernel函数外进行声明，
那么这个变量的作用域对所有kernel 函数来说都是全局的。
__shared__ float tile[size_y][size_x];

如果共享内存的大小在编译时是未知的，那么可以用 extern 关键字声明一个未知大小的数组。
例如，下面的代码段声明了共享内存一个未知大小的一维整型数组。
这个声明可以在某一个kernel函数的内部或所有kernel函数的外部进行。
extern __shared__ int tile[];
因为这个数组的大小在编译时是未知，所以在每个kernel函数被调用时，需要动态分配共享内存，
将所需的大小按字节数作为三重括号内的第三个参数，如下所示：
kernel <<<grid,block,size*sizeof(int)>>>()
请注意：只能动态声明一维数组。

5.1.3 共享内存存储体和访问模式
优化内存性能时要度量的两个关键属性是：延迟和带宽。
第4章解释了由不同的全局内存访问模式引起的延迟和带宽对kenrel函数性能的影响。
共享内存可以用来隐藏全局内存延迟和带宽对性能的影响。要想充分理解这些资源，了解共享内存是如何被安排的，对其将会有所帮助。

5.1.3.1 内存存储体
为了获得高内存带宽，共享内存被分为32个同样大小的内存模型，它们被称为存储体，它们可以被同时访问。
有32个存储体是因为在一个wrap中有32个线程。共享内存是一个一维地址空间。
根据 GPU 的计算能力，共享内存的地址在不同模式下会被映射到不同的存储体中（稍后详细描述）。
如果通过线程束发布共享内存 load 或 store 操作，且在每个存储体上只访问不多于一个的内存地址，
那么该操作可由一个内存事务来完成。否则，该操作由多个内存事务来完成，这样就降低了内存带宽的利用率。

5.1.3.2 存储体冲突
在共享内存中当多个地址请求落在相同的内存存储体中，就会发生存储体冲突，这会导致请求被重复执行。
硬件会将存储体冲突的请求分割到尽可能多的无冲突事务中，有效带宽的降低是由一个等同于所需的独立内存事务数量的因素导致的。
当线程束发出共享内存请求时，有以下三种典型的模式：
1.并行访问：多个地址访问多个存储体；
2.串行访问：多个地址访问同一个存储体；
3.广播访问：单一地址读取单一存储体；
并行访问是最常见的模式，它是被一个线程束访问的多个地址落在多个存储体中。这种模式意味着，如果不是所有的地址，
那么至少有一些地址可以在一个单一的内存事务中被服务。最佳情况是，当每个地址都位于一个单独的存储体中时，
执行无冲突的共享内存访问。

串行访问是最坏的模式，当多个地址属于同一个存储体时，必须以串行的方式进行请求。
如果线程束中 32 个线程全都访问同一个存储体中不同的内存地址，那么将需要32个内存事务，
并且满足这些访问所消耗的时间是单一请求的 32 倍。

在广播访问的情况下，线程束中所有的线程都读取统一存储体内相同的地址。
若一个内存事务被执行，那么被访问的字就会被广播到所有请求的线程中。
虽然一个单一的内存事务只需要一个广播访问，但是因为只有一小部分字节被读取，所以带宽利用率很差。

图5-2显示了最优的并行访问模式。每个线程访问一个32位字。因为每个线程访问不同存储体的地址，所以没有存储体冲突：
Thread: 0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30|31
Bank:   0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30|31

0->0;1->1;2->2;3->3;4->4;5->5;6->6;7->7;8->8;9->9;10->10;11->11;12->12;13->13;14->14;15->15;16->16;
17->17;18->18;19->19;20->20;21->21;22->22;23->23;24->24;25->25;26->26;27->27;28->28;29->29;30->30;31->31
                                   图 5-2

图5-3显示了不规则的随机访问模式。因为每个线程访问不同的存储体，所以也没有存储体冲突：
Thread: 0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30|31
Bank:   0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30|31

0->0;1->2;2->1;3->4;4->5;5->6;6->7;7->8;8->3;9->13;10->9;11->10;12->11;13->9;14->15;15->14;16->18;
17->17;18->19;19->16;20->21;21->22;22->20;23->25;24->23;25->26;26->28;27->27;28->29;29->31;30->24;31->30
                                   图 5-3

图5-4显示了另一种不规则的访问模式，在这里几个线程访问同一存储体。对于这样一个请求，会产生两种可能的行为：
1. 如果线程访问同一个存储体中相同的地址，广播访问无冲突
2. 如果线程访问同一个存储体中不同的地址，会发生存储体冲突。

Thread: 0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30|31
Bank:   0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30|31

0->0;1->0;2->0;3->0;4->0;5->0;6->0;7->0;8->8;9->8;10->8;11->8;12->8;13->8;14->8;15->8;16->16;
17->16;18->16;19->16;20->16;21->16;22->16;23->16;24->24;25->24;26->24;27->24;28->24;29->24;30->24;31->24
                                   图 5-4

5.1.3.3 访问模式
共享内存存储体的宽度规定了共享内存地址和共享内存存储体的对应关系。内存存储体的宽度随设备计算能力的不同而变化：
计算能力为2.x的设备中为4字节（32位）
计算能力为3.x的设备中为8字节（64位）

对于 Fermi 设备，存储体的宽度是 32 位并且有32个存储体。每个存储体在每两个时钟周期内都有32位的带宽。连续的32位字映射到连续的存储体中。
因此从共享内存地址到存储体索引的映射可以按如下公式进行计算：
存储体索引 = （字节地址/4字节 per 存储体）% 32存储体
字节地址除以4转换一个4字节索引，然后进行模32操作，将4字节索引转换为存储体索引。
图5-5所示的上部显示了在 Fermi设备中从字节地址到字索引的映射。
下部展示了从字索引到存储体索引的映射。注意，线程束成员访问存储体相差32字。邻近的字被分到不同的存储体中，
以最大限度地提高线程束中可能的并发访问数量。

当来自相同线程束的两个线程访问相同的地址时，不会发生存储体冲突。在这种情况下，对于读访问，这个字被广播到请求的线程中；
对于写访问，这个字只能由其中一个线程写入，执行这个写入操作的线程是不确定的：

字节地址：          0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60
4字节索引：         0 1 2 3  4  5  6  7  8  9  10 11 12 13 14 15

存储体(Bank)索引    0   1   2   3   4   5   6   7   8   9   10   11   12   13   14   15  ...  28  29  30  31
4字节索引：         0   1   2   3   4   5   6   7   8   9   10   11   12   13   14   15  ...  28  29  30  31
4字节索引：         32  33  34  35  36  37  38  39  40  41  42   43   44   45   46   47  ...  60  61  62  63
4字节索引：         64  65  66  67  68  69  70  71  72  73  74   75   76   77   78   79  ...  92  93  94  95
4字节索引：         96  97  98  99  100 101 102 103 104 105 106  107  108  109  110  111 ...  124 125 126 127
                                            图 5-5

对于 Kepler 设备，共享内存有 32 个存储体，它们有以下两种地址模式：
1. 64位模式
2. 32位模式

在 64 位模式下，连续的 64 位字映射到连续的存储体中。在每时钟周期内每个存储体都有 64 位的带宽。 
从共享内存地址到存储体索引的映射可以按以下公式进行计算：

存储体索引 = (字节地址/8字节 per 存储体) % 32 存储体

如果两个线程访问同一个 64 位字中的任何子字，从线程束发出的共享内存请求就不会产生存储体冲突，因为满足这两个请求只需要一个64位的读操作。
因此，在相同的访问模式下，相比于 Fermi 架构，在 Kepler 架构上，64位模式总是产生相同或更少的存储体冲突。

在 32 位模式下，连续的 32 位字映射到连续的存储体中。然而，因为 Kepler 在每个时钟周期内都有 64 位的带宽，
在同一存储体中访问两个 32 位字并不总意味着重操作。在单一的时钟周期内读 64 位并只将 32 位请求传输给每个线程，这是有可能的。
图 5-6 显示了在 32 位模式下，从字节地址到存储体索引的映射。上部的图是字节地址和 4 字节索引标记的共享内存。
下部的图是显示了从4字节索引到存储体索引的映射。虽然 word 0 和 word 32 都在 bank 0 中，但是在相同的内存请求中读取这两个字不会发生存储体冲突。


字节地址：          0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60
4字节索引：         0 1 2 3  4  5  6  7  8  9  10 11 12 13 14 15

存储体(Bank)索引       0       1       2       3       4       5    ...  28     29         30      31
4字节索引：         0  |32  1  |33  2  |34  3  |35  4  |36  5  |37  ...  28 |60  29 |61  30 |62  31 |63
4字节索引：         64 |96  65 |97  66 |98  67 |99  68 |100 69 |101 ...  92 |124 93 |125 94 |126 95 |127
4字节索引：         128|160     
4字节索引：         192|224    
                                            图 5-6

图 5-7 显示了在 64 位模式下无冲突访问的一种情况，在这种情况下，每个线程访问不同的存储体。
存储体(Bank)索引       0       1       2       
4字节索引：         0  |32  1  |33  2  |34  
4字节索引：         64 |96  65 |97  66 |98 
4字节索引：         128|160 129|161 130|162  
4字节索引：         192|224 193|225 194|226

Thread 0：访问 0； Thread 1: 访问 65；Thread 2: 访问 226
                图 5-7

图 5-8 显示了在 64 位模式下无冲突访问的另一种情况，在这种情况下，两个线程访问相同存储体中的字和相同的8字节的字。

存储体(Bank)索引       0       1       2       
4字节索引：         0  |32  1  |33  2  |34  
4字节索引：         64 |96  65 |97  66 |98 
4字节索引：         128|160 129|161 130|162  
4字节索引：         192|224 193|225 194|226

Thread 0：访问 0； Thread 1/2: 访问 33；Thread 3: 访问 2; Thread 4: 访问34
                图 5-8

图 5-9 展示了一个双向存储体冲突，在这种情况下，两个线程访问同一个存储体，但地址落在不同的8字节的字中。
存储体(Bank)索引       0       1       2       
4字节索引：         0  |32  1  |33  2  |34  
4字节索引：         64 |96  65 |97  66 |98 
4字节索引：         128|160 129|161 130|162  
4字节索引：         192|224 193|225 194|226

Thread 0：访问 0； Thread 1: 访问 33；Thread 2: 访问 97; Thread 3: 访问 2 Thread 4: 访问34
                图 5-9
图5-10 展示了一个三向存储体冲突，在这种情况下，3个线程访问相同的存储体，并且地址落在3个不同的8字节的字中。
存储体(Bank)索引       0       1       2       
4字节索引：         0  |32  1  |33  2  |34  
4字节索引：         64 |96  65 |97  66 |98 
4字节索引：         128|160 129|161 130|162  
4字节索引：         192|224 193|225 194|226

Thread 0：访问 0； Thread 1: 访问 1；Thread 2: 访问 97; Thread 3: 访问 129 Thread 4: 访问34
                图 5-10

5.1.3.4 内存填充
内存填充是避免存储体冲突的一种方法。在图 5-11 所示位通过一个简单的例子来说明内存填充。
假设只有5个共享内存的存储体。如果所有线程访问 bank 0 中的不同地址，那么会发生一个5向的存储体冲突。
解决这种存储体冲突的一个方法是在每 N 个元素之后添加一个字，这里的 N 是存储体的数量。这就改变了从字到存储体的映射。
如图 5-11 的下侧所示。由于填充，之前所有属于bank 0 的字就会被传播到不同的存储体中。

填充的内存不能用于数据的存储。其唯一的作用就是移动数据元素，以便将原来属于同一个存储体中的数据分散到不同存储体中。
这样线程块可用的总的共享内存的数量将会减少。填充之后，还需要重新计算数组索引以确保可以访问到正确的数据元素。
虽然在 Fermi 和 Kepler 都有 32 个存储体，但是它们的存储体的宽度不同。在这些不同的架构上填充共享内存时，必须要小心。
Fermi 架构中的某些内存填充模式可能会导致 Kepler 中的存储体冲突。

存储体(Bank)索引    0   1   2   3   4   F（fill，填充）
4字节索引：         0   1   2   3   4   F 
4字节索引：         0   1   2   3   4   F
4字节索引：         0   1   2   3   4   F 
4字节索引：         0   1   2   3   4   F
4字节索引：         0   1   2   3   4   F

存储体(Bank)索引    0   1   2   3   4 
--------------------------------------  
4字节索引：         0   1   2   3   4    
4字节索引：         F   0   1   2   3   
4字节索引：         4   F   0   1   2    
4字节索引：         3   4   F   0   1   
4字节索引：         2   3   4   F   0 
4字节索引：         1   2   3   4   F 

5.1.3.5 访问模式配置
之前提到过，Kepler 设备支持 4 字节和 8 字节的共享内存访问模式。
默认是4字节模式。可以采用以下的 CUDA Runtime API 函数查询访问模式：
cudaError_t cudaDeviceGetSharedMemConfig(cudaSharedMemConfig *pConfig);
结果返回到 pConfig 中。返回的存储体配置可以是下列值中的一个：
cudaSharedMemBankSizeFourByte
cudaSharedMemBankSizeEightByte

在可配置共享内存存储体的设备上，可以使用以下功能设置一个新的存储体的大小：
cudaError_t cudaDeviceSetSharedMemConfig(cudaSharedMemConfig config);
支持的存储体配置为：
cudaSharedMemBankSizeDefault;
cudaSharedMemBankSizeFourByte;
cudaSharedMemBankSizeEightByte

在不同的kernel函数启动之间更改共享内存配置可能需要一个隐式的设备同步点。更改共享内存存储体的大小不会增加共享内存的使用量，
也不会影响kernel函数的占用率，但它对性能可能有重大影响。一个大的存储体可能为共享内存访问产生更高的带宽，但是可能导致更多的存储体冲突。
这取决于应用程序中共享内存的访问模式。

5.1.4 配置共享内存量
每个 SM 都有 64 KB 的片上内存。共性内存和一级缓存共享该硬件资源。 CUDA 为配置 L1 Cache 和 shared memory 的大小提供了两种方法：
1. 按设备进行配置；
2. 按kernel函数进行配置。
谁用下述的运行时函数，可以为设备上启动的kernel函数配置 L1 Cache 和 shared memory的大小：
cudaError_t cudaDeviceSetCacheConfig(cudaFuncCache chaceConfig);

参数 cacheConfig 指明，在当前的 CUDA 设备上，片上内存是如何在 L1 Cache 和 shared memory间进行划分的。
所支持的缓存配置参数如下所示：
cudaFuncCachePreferNone: no perference (default)
cudaFuncCachePreferShared: prefer 48KB shared memory and 16KB L1 Cache;
cudaFuncCachePreferL1: prefer 48KB L1 Cache and 16KB shared memory;
cudaFuncCachePreferEqual: prefer 32KB L1 Cache and 32KB shared memory
哪种模式更好，这取决于在kernel函数中使用了多少共享内存，典型情况如下：
1. 当kernel函数使用较多的共享内存时，倾向于更多的共享内存
2. 当kernel函数使用更多的寄存器时，倾向于更多的一级缓存。
如果kernel函数使用了大量的共享内存，那么配置 48KB 的共享内存能实现较高的占用率和更好的性能。
另一方面，如果kernel函数仅使用了少量的共享，那么一级缓存配置cacheConfig参数为48KB。
对 Kepler 设备而言，一级缓存用于寄存器溢出。指定 -Xptxas -v 选项给 nvcc 可以知道kenrel函数使用了多少寄存器。
当kernel函数使用的寄存器数量超过了硬件限制所允许的数量时，应该为寄存器溢出配置一个更大的 L1 Cache。
对于 Fermi 设备而言，本地内存用于溢出寄存器，但本地内存的加载可能被缓存在一级缓存中。在这种情况下，
大的一级缓存可能也是有益的。

CUDA Runtime 会尽可能使用请求设备的片上内存配置，但如果需要执行一个kenrel函数，它可以自由地选择不同的配置。
每个kernel函数的配置可以覆盖设备范围的设置，也可以使用以下runtime 函数进行设置：
cudaError_t cudaFuncSetCacheConfig(const void *func,enum cudaFuncCacheca cheConfig);
kernel 函数使用的这种配置是由kenrel函数指针func指定的。
启动一个不同优先级的内核比启动有最近优先级设置的内核更可能导致隐式设备同步。
对于每个kernel，只需调用一次这个函数。每个kernel函数启动时，片上内存中的配置不需要重新设定。

虽然一级缓存和共享内存位于相同的片上硬件中，但在某些方面它们却不太相同。
共享内存是通过32个存储体进行访问的，而一级缓存则是通过缓存行进行访问。使用共享内存，对存储内容和存放位置有完全的控制权。
而使用一级缓存，数据删除工作是由硬件完成的。

CPU 缓存与GPU 缓存 
一般情况下，GPU 缓存的行为比 CPU 缓存的行为更加难以理解。 GPU 使用不同的启发式算法删除数据。
在 GPU 上，数百个线程共享相同的一级缓存，数千个线程共享相同的二级缓存。
因此，数据删除在 GPU 上可能会发生得更频繁而且更不可预知。
使用 GPU 共享内存不仅可以显式地管理数据而且还可以保证 SM 的局部性。

5.1.5 同步
并行线程间的同步是所有并行计算语言的重要机制。正如它名字所暗示的，共享内存可以同时被block中的多个线程访问。
当不同步的多个线程修改同一个共享内存地址时，将导致线程内的冲突。CUDA 提供了几个runtime函数来执行block内的同步。
同步的两个基本方法如下所示：
1. 障碍
2. 内存栅栏
在障碍中，所有调用的线程等待其余调用的线程到达障碍点。
在内存栅栏中，所有调用的线程必须等到全部内存修改对其余调用线程可见时才能继续执行。
然而，在学习 CUDA 的块内障碍点和内存栅栏之前，理解 CUDA 采用的弱排序内存模型是十分重要的。

5.1.5.1 弱排序内存模型
现代的内存架构有一个宽松的内存模型。这意味着，内存访问不一定要按照它们在程序中出现的顺序执行。
CUDA 采用弱排序内存模型从而优化了更多激进的编译器。
GPU 线程在不同内存（如共享内存、全局内存、锁页主机内存或对等设备的内存）中写入数据的顺序，不一定和
这些数据在源代码中访问的顺序相同。一个线程的写入顺序对其他线程可见时，它可能和写操作被执行的实际顺序不一致。
如果指令之间是相互独立的，线程从不同内存中读取数据的顺序和读指令在程序中出现的顺序不一定相同。
为了显式地强制程序以一个确切的顺序执行，必须在应用程序代码中插入内存栅栏和障碍点。
这是保证与其他线程共享资源的kernel函数行为正确的唯一途径。

5.1.5.2 显式障碍
在 CUDA 中，障碍点只能在同一block的线程间执行。在kernel函数中，可以通过调用下面的函数来指定一个障碍点：
void __syncthreads();

__syncthreads作为一个障碍点来发挥作用，它要求block中的线程必须等待直到所有线程都到达该点。
__syncthreads还确保在障碍点之前，被这些线程访问的所有全局和共享内存对同一块中的所有线程都可见。
__syncthreads用于协调同一block中线程间的通信。当block中的某些线程访问共享内存或全局内存中的同一个地址时，
会有潜在的问题（写后读、读后写、写后写），这将导致在那些内存位置产生未定义的应用程序行为和未定义的状态。
可以通过利用冲突访问间的同步线程避免这种情况。
在条件代码中使用 __syncthreads 时，必须要特别小心。如果一个条件能保证对整个block进行同等评估（都能到达障碍点，或都不到达障碍点），
则它是调用 __syncthreads 的唯一有效条件。否则执行很可能会挂起或产生意想不到的问题。
例如，下面的代码可能会导致块中的线程无限期地等待对方，因为块中的所有线程没有到达相同的障碍点：

if(threadID % 2 == 0){
    __syncthreads();
}else{
    __syncthreads();
}

如果不允许跨线程块同步，线程块可能会以任何顺序、并行、串行的顺序在任何 SM 上执行。
block 执行的独立性使得 CUDA 编程在任意数量的 kernel 中都是可扩展的。
如果一个 CUDA kernel 函数要求跨线程块全局同步，那么通过在同步点分割kernel 函数并执行多个kernel启动可能会达到预期的效果。
因为每个连续的kernel 启动必须等待之前的kernel启动完成，所以这会产生一个隐式全局障碍。

5.1.5.3 内存栅栏
内存栅栏的功能可确保栅栏前的任何内存写操作对栅栏后的其他线程都是可见的。
根据所需范围，有3种内存栅栏：block、grid 或系统。
通过以下固有函数可以在线程块内创建内存栅栏：
void __threadfence_block();
__threadfence_block保证了栅栏前被调用线程产生的对共享内存和全局内存的所有写操作对栅栏后同一块中的其他线程都是可见的。
回想一下，内存栅栏不执行任何线程同步，所以对于同一个块中的所有线程来说，没有必要实际执行这个指令。

使用下面的固有函数来创建网格级栅栏：
void __threadfence();
__threadfence挂起调用的线程，直到全局内存中的所有写操作对相同网格内的所有线程都是可见的。
使用下面的函数可以跨系统（包括主机和设备）设置内存栅栏：
void __threadfence_system();

__threadfence_system挂起调用的线程，以确保该线程对全局内存、锁页主机内存和其他设备内存中的所有写操作对全部设备中的线程和主机线程都是可见的。

5.1.5.4 Volatile 修饰符
在全局或共享内存中，使用voilatile 修饰符声明一个变量，可以防止编译器优化，编译器优化可能将数据暂时缓存在寄存器或本地内存中。
当使用volatile 修饰符时，编译器假定任何其他线程在任何时间都可以更改或使用该变量的值。
因此，这个变量的任何引用都会直接被编译到全局内存读指令或全局内存写指令中，它们都会忽略缓存。

共享内存和全局内存

GPU 全局内存常驻在设备内存（DRAM）上，它比 GPU 的共享内存访问慢得多。相比于 DRAM，共享内存有以下的特点：
1. DRAM 比 shared memory 的延迟高 20~30倍
2. shared memory 比 DRAM 的带宽大10倍

共享内存的访问粒度也比较小。而 DRAM 的访问粒度可以是 32 个字节或129个字节，共享内存的访问粒度如下：
Fermi 架构：4字节存储体宽
Kepler架构：8字节存储体宽







